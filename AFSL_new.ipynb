{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashNarayan23/v-ident-app-model/blob/main/AFSL_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-sSHlbX8jSh"
      },
      "source": [
        "\n",
        "I am creating an 'identity verification systems built on pre-al assumption', in this system get data from sensors on mobile devices (like RGB Camera, IMU Accelerometer & Gyroscope, Ambient Light Sensor, Touch input) and using ML models, figure out if an actual human is using the device or is an AI pretending to be human. This verification system will have a client side ML processing module. This module will directly take the data from mobile phone sensors, preprocess them, get through following layers of checking - Physiological Extraction, Cognitive Neuromuscular coupling, Motion Cross Sensor consistency, Visual Frequency forensics and behavioral biometrics; and then give a 'Trust Score(TS)' based on the individual scores from these layers. Now tell me how should this ML model be implemented, and also go forward with implementing the 'data acquisition and preprocessing' where you would preprocess the sensor data and make it ready for ML processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40a93387"
      },
      "source": [
        "### Evaluate Model Performance on Test Data\n",
        "\n",
        "Now that the model has been trained with simulated real data, we can evaluate its performance on the separate test set (`X_test`, `y_test`). This gives an indication of how well the model generalizes to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c350fa6",
        "outputId": "f1e27b9d-8116-427b-843b-6e38c25c256f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluating Model on Test Data ---\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 750ms/step - accuracy: 0.4961 - loss: 0.7042\n",
            "Test Loss: 0.7074\n",
            "Test Accuracy: 0.5000\n",
            "\n",
            "Note: These metrics are based on simulated data and may not reflect real-world performance.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"\\n--- Evaluating Model on Test Data ---\")\n",
        "\n",
        "# Check if model, X_test, and y_test are defined before proceeding\n",
        "if 'model' in locals() and 'X_test' in locals() and 'y_test' in locals():\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "else:\n",
        "    print(\"Error: 'model', 'X_test', or 'y_test' is not defined in the current scope.\")\n",
        "    print(\"Please ensure the model has been initialized (e.g., in cell 'f6395d2c')\")\n",
        "    print(\"and the test data has been prepared (e.g., in cell '540f0f76') before running this cell.\")\n",
        "\n",
        "print(\"\\nNote: These metrics are based on simulated data and may not reflect real-world performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98d5fd9f"
      },
      "source": [
        "### Evaluate Model Performance with Dummy Data\n",
        "\n",
        "Although the model was trained on dummy data, we can still demonstrate how to evaluate its performance using common classification metrics. This involves comparing the `predictions` (probabilities) generated by the model with the `dummy_labels` (true labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e17a5f63",
        "outputId": "4934e3a5-10d9-4221-a754-b57d901b0797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of binary predictions: (150, 1)\n",
            "Shape of dummy labels: (150,)\n",
            "\n",
            "--- Model Evaluation (on Dummy Data) ---\n",
            "Accuracy: 0.5000\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1-Score: 0.0000\n",
            "\n",
            "Note: These metrics are based on random dummy data and labels, and do not reflect real-world performance.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Convert probabilities to binary predictions (0 or 1)\n",
        "# A common threshold for sigmoid output is 0.5\n",
        "binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "print(f\"Shape of binary predictions: {binary_predictions.shape}\")\n",
        "print(f\"Shape of dummy labels: {dummy_labels.shape}\")\n",
        "\n",
        "# Ensure both arrays are 1-dimensional for sklearn metrics\n",
        "binary_predictions_flat = binary_predictions.flatten()\n",
        "dummy_labels_flat = dummy_labels.flatten()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(dummy_labels_flat, binary_predictions_flat)\n",
        "precision = precision_score(dummy_labels_flat, binary_predictions_flat)\n",
        "recall = recall_score(dummy_labels_flat, binary_predictions_flat)\n",
        "f1 = f1_score(dummy_labels_flat, binary_predictions_flat)\n",
        "\n",
        "print(\"\\n--- Model Evaluation (on Dummy Data) ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nNote: These metrics are based on random dummy data and labels, and do not reflect real-world performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a54ec180"
      },
      "source": [
        "### 1. Simulate Camera Video Data (Re-run for fresh `dummy_video`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94e727a4",
        "outputId": "83e07e39-5744-440f-a9f6-45f79485f218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simulating video capture: 150 frames, 224x224x3.\n",
            "Generated video clip with 150 frames.\n",
            "Length of the simulated video (number of frames): 150\n",
            "Shape of the first frame: (224, 224, 3)\n",
            "Data type of the first frame: uint8\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def simulate_video_capture(num_frames: int = 150, frame_width: int = 224, frame_height: int = 224, num_channels: int = 3):\n",
        "    \"\"\"\n",
        "    Simulates the capture of a 5-second video clip from a mobile phone camera.\n",
        "\n",
        "    Args:\n",
        "        num_frames (int): Total number of frames in the 5-second clip (e.g., 30 fps * 5s = 150 frames).\n",
        "        frame_width (int): Width of each video frame in pixels.\n",
        "        frame_height (int): Height of each video frame in pixels.\n",
        "        num_channels (int): Number of color channels (e.g., 3 for RGB).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of NumPy arrays, where each array represents a video frame.\n",
        "              Each frame is of shape (frame_height, frame_width, num_channels)\n",
        "              with pixel values in the range [0, 255] and dtype uint8.\n",
        "    \"\"\"\n",
        "    print(f\"Simulating video capture: {num_frames} frames, {frame_width}x{frame_height}x{num_channels}.\")\n",
        "    video_clip = []\n",
        "    for _ in range(num_frames):\n",
        "        frame = np.random.randint(0, 256, size=(frame_height, frame_width, num_channels), dtype=np.uint8)\n",
        "        video_clip.append(frame)\n",
        "\n",
        "    print(f\"Generated video clip with {len(video_clip)} frames.\")\n",
        "    return video_clip\n",
        "\n",
        "# Simulate a 5-second video at 30 FPS (150 frames total) with 224x224 RGB frames\n",
        "dummy_video = simulate_video_capture(num_frames=150, frame_width=224, frame_height=224, num_channels=3)\n",
        "\n",
        "print(f\"Length of the simulated video (number of frames): {len(dummy_video)}\")\n",
        "if len(dummy_video) > 0:\n",
        "    print(f\"Shape of the first frame: {dummy_video[0].shape}\")\n",
        "    print(f\"Data type of the first frame: {dummy_video[0].dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28fb0872"
      },
      "source": [
        "### 2. Preprocess Video Data (Re-run for fresh `preprocessed_dummy_video`, `TARGET_WIDTH`, `TARGET_HEIGHT`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e070a9a4",
        "outputId": "501c9f32-1e78-42a4-fc48-3cad17caea63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Length of the preprocessed video: 150\n",
            "Shape of the first preprocessed frame: (224, 224, 3)\n",
            "Data type of the first preprocessed frame: float32\n",
            "Min pixel value of first preprocessed frame: 0.0\n",
            "Max pixel value of first preprocessed frame: 1.0\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "def preprocess_video_frames(frames: list, target_width: int, target_height: int):\n",
        "    \"\"\"\n",
        "    Preprocesses a list of video frames by resizing and normalizing pixel values.\n",
        "\n",
        "    Args:\n",
        "        frames (list): A list of NumPy arrays, where each array is a video frame.\n",
        "        target_width (int): The desired width for each preprocessed frame.\n",
        "        target_height (int): The desired height for each preprocessed frame.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of preprocessed NumPy arrays, with resized and normalized frames.\n",
        "              Each frame is of shape (target_height, target_width, num_channels)\n",
        "              with pixel values in the range [0, 1] and dtype float32.\n",
        "    \"\"\"\n",
        "    print(f\"Preprocessing {len(frames)} frames to {target_width}x{target_height} and normalizing.\")\n",
        "    preprocessed_frames = []\n",
        "    for frame in frames:\n",
        "        resized_frame = cv2.resize(frame, (target_width, target_height))\n",
        "        normalized_frame = resized_frame.astype(np.float32) / 255.0\n",
        "        preprocessed_frames.append(normalized_frame)\n",
        "\n",
        "    print(f\"Finished preprocessing. Generated {len(preprocessed_frames)} frames.\")\n",
        "    return preprocessed_frames\n",
        "\n",
        "# Define target dimensions for MobileNetV2\n",
        "TARGET_WIDTH = 224\n",
        "TARGET_HEIGHT = 224\n",
        "\n",
        "# Preprocess the dummy video\n",
        "preprocessed_dummy_video = preprocess_video_frames(dummy_video, TARGET_WIDTH, TARGET_HEIGHT)\n",
        "\n",
        "print(f\"Length of the preprocessed video: {len(preprocessed_dummy_video)}\")\n",
        "if len(preprocessed_dummy_video) > 0:\n",
        "    print(f\"Shape of the first preprocessed frame: {preprocessed_dummy_video[0].shape}\")\n",
        "    print(f\"Data type of the first preprocessed frame: {preprocessed_dummy_video[0].dtype}\")\n",
        "    print(f\"Min pixel value of first preprocessed frame: {preprocessed_dummy_video[0].min()}\")\n",
        "    print(f\"Max pixel value of first preprocessed frame: {preprocessed_dummy_video[0].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4be2b2c5"
      },
      "source": [
        "### 3. Implement Deepfake Model (Re-run for fresh `model`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "c585f8d5",
        "outputId": "924a7a9c-6592-4f67-ecb4-b2330b7bbf8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Architecture Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ mobilenetv2_1.00_224            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling2d_2      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ mobilenetv2_1.00_224            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     â”‚     \u001b[38;5;34m2,257,984\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)                    â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling2d_2      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m163,968\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚           \u001b[38;5;34m129\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,081</span> (9.24 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,081\u001b[0m (9.24 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,097</span> (641.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,097\u001b[0m (641.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input shape for the model\n",
        "input_shape = (TARGET_HEIGHT, TARGET_WIDTH, 3)\n",
        "\n",
        "# Load the pre-trained MobileNetV2 model without its top classification layer\n",
        "base_model = MobileNetV2(input_shape=input_shape,\n",
        "                           include_top=False,\n",
        "                           weights='imagenet')\n",
        "\n",
        "# Freeze the layers of the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create a new model by defining the input layer and then chaining the base_model\n",
        "inputs = Input(shape=input_shape)\n",
        "x = base_model(inputs, training=False)\n",
        "\n",
        "# Add custom classification layers on top of the base_model's output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Instantiate the full Keras Model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Model Architecture Summary:\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94c1d09d"
      },
      "source": [
        "### 4. Prepare Video Data for Model Input (Re-run for fresh `X_test_video`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a977d60",
        "outputId": "a26d25a0-245f-4855-b0b1-ed2515b51d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the prepared video data for model input: (150, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert the list of preprocessed frames into a single NumPy array\n",
        "# The shape should be (number_of_frames, TARGET_HEIGHT, TARGET_WIDTH, 3)\n",
        "X_test_video = np.array(preprocessed_dummy_video)\n",
        "\n",
        "print(f\"Shape of the prepared video data for model input: {X_test_video.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c6de2ae"
      },
      "source": [
        "### 5. Get Predictions on Dummy Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4c975d0",
        "outputId": "bcab38c3-5abe-422c-e449-310c2eb64f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step\n",
            "Shape of the model predictions: (150, 1)\n",
            "First 10 predictions:\n",
            "[[0.76093316]\n",
            " [0.815377  ]\n",
            " [0.7539271 ]\n",
            " [0.7753731 ]\n",
            " [0.804504  ]\n",
            " [0.7881731 ]\n",
            " [0.76163334]\n",
            " [0.75020695]\n",
            " [0.8032901 ]\n",
            " [0.7630683 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Use the model.predict() method to get predictions for the prepared video data\n",
        "predictions = model.predict(X_test_video)\n",
        "\n",
        "# Print the shape of the predictions\n",
        "print(f\"Shape of the model predictions: {predictions.shape}\")\n",
        "\n",
        "# Display the first few predictions\n",
        "print(\"First 10 predictions:\")\n",
        "print(predictions[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fba101e",
        "outputId": "ccdfdd7a-cf5c-448e-e38b-6d2d7dd5f02e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 857ms/step\n",
            "Shape of the model predictions: (150, 1)\n",
            "First 10 predictions:\n",
            "[[0.76093316]\n",
            " [0.815377  ]\n",
            " [0.7539271 ]\n",
            " [0.7753731 ]\n",
            " [0.804504  ]\n",
            " [0.7881731 ]\n",
            " [0.76163334]\n",
            " [0.75020695]\n",
            " [0.8032901 ]\n",
            " [0.7630683 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Use the model.predict() method to get predictions for the prepared video data\n",
        "predictions = model.predict(X_test_video)\n",
        "\n",
        "# Print the shape of the predictions\n",
        "print(f\"Shape of the model predictions: {predictions.shape}\")\n",
        "\n",
        "# Display the first few predictions\n",
        "print(\"First 10 predictions:\")\n",
        "print(predictions[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a96e2a8e"
      },
      "source": [
        "### How to Use an API Key in Google Colab\n",
        "\n",
        "1.  **Store Your API Key in Colab Secrets**:\n",
        "    *   Click the 'ğŸ”‘' (Secrets) icon on the left sidebar.\n",
        "    *   Add a new secret. For example, name it `MY_API_KEY` and paste your API key as its value.\n",
        "    *   Ensure 'Notebook access' is enabled for this notebook.\n",
        "\n",
        "2.  **Access the API Key in Your Code**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78c0c829",
        "outputId": "0166310d-fb6f-49d5-e629-4fa469ad9565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 150 dummy labels with shape: (150,)\n",
            "First 10 dummy labels: [1 1 1 1 0 0 1 1 0 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# For demonstration purposes, let's create dummy labels for our simulated video data.\n",
        "# In a real scenario, these labels would come from your actual dataset (e.g., 0 for real, 1 for deepfake).\n",
        "# We'll assume a binary classification task.\n",
        "# Let's say, for example, the first half of the frames are 'real' (label 0) and the second half are 'deepfake' (label 1).\n",
        "\n",
        "num_frames = len(preprocessed_dummy_video)\n",
        "dummy_labels = np.array([0] * (num_frames // 2) + [1] * (num_frames - num_frames // 2))\n",
        "\n",
        "# If you want to shuffle the labels for a more realistic dummy dataset:\n",
        "np.random.shuffle(dummy_labels)\n",
        "\n",
        "print(f\"Generated {len(dummy_labels)} dummy labels with shape: {dummy_labels.shape}\")\n",
        "print(\"First 10 dummy labels:\", dummy_labels[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50cfc345"
      },
      "source": [
        "Now that we have dummy data and dummy labels, we can demonstrate how to train the model using `model.fit()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb221ef8",
        "outputId": "a386723f-99b1-45b6-9357-8f6aef870231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting model training with 5 epochs and batch size 32...\n",
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.5671 - loss: 0.8993 - val_accuracy: 0.5333 - val_loss: 1.0168\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.4940 - loss: 0.9790 - val_accuracy: 0.4667 - val_loss: 0.9067\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.4633 - loss: 0.8644 - val_accuracy: 0.5333 - val_loss: 0.7077\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.4919 - loss: 0.7158 - val_accuracy: 0.5000 - val_loss: 0.7006\n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5283 - loss: 0.7063 - val_accuracy: 0.4667 - val_loss: 0.6858\n",
            "\n",
            "Model training demonstration complete!\n"
          ]
        }
      ],
      "source": [
        "# Ensure the features (X_test_video) and labels (dummy_labels) are in the correct format\n",
        "# X_test_video is already a NumPy array of shape (num_frames, H, W, C)\n",
        "# dummy_labels is a NumPy array of shape (num_frames,)\n",
        "\n",
        "# Define training parameters (these are example values and should be tuned for real training)\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "print(f\"Starting model training with {epochs} epochs and batch size {batch_size}...\")\n",
        "\n",
        "# Train the model\n",
        "# In a real scenario, you would split your data into training and validation sets.\n",
        "# For this demonstration, we'll use the entire dummy dataset for a quick fit.\n",
        "history = model.fit(\n",
        "    x=X_test_video, # Your preprocessed video frames\n",
        "    y=dummy_labels, # Corresponding labels (real/deepfake)\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2, # Use 20% of the data for validation during training\n",
        "    verbose=1 # Show training progress\n",
        ")\n",
        "\n",
        "print(\"\\nModel training demonstration complete!\")\n",
        "\n",
        "# You can inspect the training history\n",
        "# print(history.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf2b6f92"
      },
      "source": [
        "### Important Note on Training:\n",
        "\n",
        "This training demonstration uses **simulated data and dummy labels**. For the model to effectively detect deepfakes, you must:\n",
        "\n",
        "1.  **Acquire Real-world Deepfake Datasets**: Obtain a diverse dataset containing both genuine video clips and deepfake videos, each accurately labeled.\n",
        "2.  **Robust Data Preprocessing**: Ensure your preprocessing steps are consistent and optimized for your real data.\n",
        "3.  **Split Data**: Divide your dataset into training, validation, and test sets to properly evaluate the model's performance and prevent overfitting.\n",
        "4.  **Hyperparameter Tuning**: Experiment with different epochs, batch sizes, optimizers, and learning rates.\n",
        "5.  **AFSL Capabilities**: Once the base model is performing well, you can explore strategies for Few-Shot Learning by carefully structuring your dataset and potentially modifying the model's head for adaptation to new, unseen deepfake types with limited examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l70nEC54CIfA"
      },
      "source": [
        "I am creating an 'identity verification systems built on pre-AI assumption'. In this, system gets two datasets from mobile phone's camera, one as normal 5 second video-clip to detect deepfake and other one as 5 second video-clip taken live after a flash of white on screen (to detect human eye's pupil dilation on bright light flash). These datasets need to be preprocessed appropriately for further ML processing. For now you may create functions which will simulate creation of these sensor data, but make sure you have implemented actual sensor data handling in the data acquisition part. Then after this, implement AFSL model using MobileNetV2 to detect deepfake to begin with.\n",
        "\n",
        "(Remove pupil dilation detection part, it won't work)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be6c0852"
      },
      "source": [
        "# Task\n",
        "Simulate the capture of 5-second video clips from a mobile phone camera, generate dummy video data suitable for deepfake detection, preprocess this simulated video data for a deepfake detection model (extract frames, resize to 224x224, normalize pixel values), and then implement an AFSL deepfake detection model using MobileNetV2 as a feature extractor with custom classification layers. Integrate the preprocessed simulated data with the model to demonstrate its usage and prepare for future integration with actual mobile sensor data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77a54972"
      },
      "source": [
        "## Simulate Camera Video Data\n",
        "\n",
        "### Subtask:\n",
        "Create Python functions that simulate the capture of 5-second video clips from a mobile phone camera. These functions should generate dummy video data (e.g., using NumPy or OpenCV to create synthetic frames) that mimics real camera input in terms of dimensions and frame rate, suitable for deepfake detection. This step lays the groundwork for later integration with actual sensor data, ensuring the data acquisition pipeline is robust.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62f9bbc5"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to simulate camera video data. I will create a Python function `simulate_video_capture` that generates dummy video frames using NumPy, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22b76620",
        "outputId": "354535e4-5376-4499-ee4e-f202ef4012ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simulating video capture: 150 frames, 224x224x3.\n",
            "Generated video clip with 150 frames.\n",
            "Length of the simulated video (number of frames): 150\n",
            "Shape of the first frame: (224, 224, 3)\n",
            "Data type of the first frame: uint8\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def simulate_video_capture(num_frames: int = 150, frame_width: int = 224, frame_height: int = 224, num_channels: int = 3):\n",
        "    \"\"\"\n",
        "    Simulates the capture of a 5-second video clip from a mobile phone camera.\n",
        "\n",
        "    Args:\n",
        "        num_frames (int): Total number of frames in the 5-second clip (e.g., 30 fps * 5s = 150 frames).\n",
        "        frame_width (int): Width of each video frame in pixels.\n",
        "        frame_height (int): Height of each video frame in pixels.\n",
        "        num_channels (int): Number of color channels (e.g., 3 for RGB).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of NumPy arrays, where each array represents a video frame.\n",
        "              Each frame is of shape (frame_height, frame_width, num_channels)\n",
        "              with pixel values in the range [0, 255] and dtype uint8.\n",
        "    \"\"\"\n",
        "    print(f\"Simulating video capture: {num_frames} frames, {frame_width}x{frame_height}x{num_channels}.\")\n",
        "    video_clip = []\n",
        "    for _ in range(num_frames):\n",
        "        # Generate random pixel values for each frame\n",
        "        # Using np.random.randint to get integer values between 0 and 255\n",
        "        frame = np.random.randint(0, 256, size=(frame_height, frame_width, num_channels), dtype=np.uint8)\n",
        "        video_clip.append(frame)\n",
        "\n",
        "    print(f\"Generated video clip with {len(video_clip)} frames.\")\n",
        "    return video_clip\n",
        "\n",
        "# Example usage to demonstrate the function\n",
        "# Simulate a 5-second video at 30 FPS (150 frames total) with 224x224 RGB frames\n",
        "dummy_video = simulate_video_capture(num_frames=150, frame_width=224, frame_height=224, num_channels=3)\n",
        "\n",
        "print(f\"Length of the simulated video (number of frames): {len(dummy_video)}\")\n",
        "if len(dummy_video) > 0:\n",
        "    print(f\"Shape of the first frame: {dummy_video[0].shape}\")\n",
        "    print(f\"Data type of the first frame: {dummy_video[0].dtype}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf39249c"
      },
      "source": [
        "## Preprocess Video Data for Deepfake Detection\n",
        "\n",
        "### Subtask:\n",
        "Develop functions to preprocess the simulated video data. This involves extracting individual frames from the video clips, resizing them to a standard input dimension (e.g., 224x224 pixels, suitable for MobileNetV2), and normalizing pixel values (e.g., to the range [0, 1] or [-1, 1]). These steps prepare the data for consumption by the deepfake detection model, ensuring consistent input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19d3303e"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to preprocess the video data by defining a function that extracts, resizes, and normalizes frames. I will create a Python function `preprocess_video_frames` that takes a list of frames, resizes them to the specified dimensions, and normalizes pixel values to the range [0, 1] using OpenCV, and then apply it to the `dummy_video`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5fda834",
        "outputId": "150b4827-64cd-481d-ed57-ec838527c006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Length of the preprocessed video: 150\n",
            "Shape of the first preprocessed frame: (224, 224, 3)\n",
            "Data type of the first preprocessed frame: float32\n",
            "Min pixel value of first preprocessed frame: 0.0\n",
            "Max pixel value of first preprocessed frame: 1.0\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "def preprocess_video_frames(frames: list, target_width: int, target_height: int):\n",
        "    \"\"\"\n",
        "    Preprocesses a list of video frames by resizing and normalizing pixel values.\n",
        "\n",
        "    Args:\n",
        "        frames (list): A list of NumPy arrays, where each array is a video frame.\n",
        "        target_width (int): The desired width for each preprocessed frame.\n",
        "        target_height (int): The desired height for each preprocessed frame.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of preprocessed NumPy arrays, with resized and normalized frames.\n",
        "              Each frame is of shape (target_height, target_width, num_channels)\n",
        "              with pixel values in the range [0, 1] and dtype float32.\n",
        "    \"\"\"\n",
        "    print(f\"Preprocessing {len(frames)} frames to {target_width}x{target_height} and normalizing.\")\n",
        "    preprocessed_frames = []\n",
        "    for frame in frames:\n",
        "        # Resize the frame\n",
        "        resized_frame = cv2.resize(frame, (target_width, target_height))\n",
        "\n",
        "        # Normalize pixel values to [0, 1]\n",
        "        normalized_frame = resized_frame.astype(np.float32) / 255.0\n",
        "        preprocessed_frames.append(normalized_frame)\n",
        "\n",
        "    print(f\"Finished preprocessing. Generated {len(preprocessed_frames)} frames.\")\n",
        "    return preprocessed_frames\n",
        "\n",
        "# Define target dimensions for MobileNetV2\n",
        "TARGET_WIDTH = 224\n",
        "TARGET_HEIGHT = 224\n",
        "\n",
        "# Preprocess the dummy video generated in the previous step\n",
        "preprocessed_dummy_video = preprocess_video_frames(dummy_video, TARGET_WIDTH, TARGET_HEIGHT)\n",
        "\n",
        "print(f\"Length of the preprocessed video: {len(preprocessed_dummy_video)}\")\n",
        "if len(preprocessed_dummy_video) > 0:\n",
        "    print(f\"Shape of the first preprocessed frame: {preprocessed_dummy_video[0].shape}\")\n",
        "    print(f\"Data type of the first preprocessed frame: {preprocessed_dummy_video[0].dtype}\")\n",
        "    print(f\"Min pixel value of first preprocessed frame: {preprocessed_dummy_video[0].min()}\")\n",
        "    print(f\"Max pixel value of first preprocessed frame: {preprocessed_dummy_video[0].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5de6496d"
      },
      "source": [
        "## Implement AFSL Deepfake Model with MobileNetV2\n",
        "\n",
        "### Subtask:\n",
        "Implement the core deepfake detection model using a pre-trained MobileNetV2 as a feature extractor and add custom classification layers for deepfake detection. The model should be structured to be adaptable for few-shot learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48061b8a"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to implement an AFSL deepfake detection model using MobileNetV2. I will start by importing the necessary libraries, defining the model architecture with MobileNetV2 as a feature extractor, adding custom classification layers, freezing the base model, and then compiling and summarizing the model as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "f6395d2c",
        "outputId": "6141cca1-17ad-40e8-b8f9-06fae9138420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Architecture Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ mobilenetv2_1.00_224            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling2d_3      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ mobilenetv2_1.00_224            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     â”‚     \u001b[38;5;34m2,257,984\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)                    â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling2d_3      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m163,968\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚           \u001b[38;5;34m129\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,081</span> (9.24 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,081\u001b[0m (9.24 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,097</span> (641.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,097\u001b[0m (641.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 2. Define the input shape for the model\n",
        "# TARGET_HEIGHT and TARGET_WIDTH are already defined in previous cells\n",
        "input_shape = (TARGET_HEIGHT, TARGET_WIDTH, 3)\n",
        "\n",
        "# 3. Load the pre-trained MobileNetV2 model without its top classification layer\n",
        "base_model = MobileNetV2(input_shape=input_shape,\n",
        "                           include_top=False,\n",
        "                           weights='imagenet')\n",
        "\n",
        "# 4. Freeze the layers of the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# 5. Create a new model by defining the input layer and then chaining the base_model\n",
        "inputs = Input(shape=input_shape)\n",
        "x = base_model(inputs, training=False)\n",
        "\n",
        "# 6. Add custom classification layers on top of the base_model's output\n",
        "# First, add a GlobalAveragePooling2D layer to flatten the feature maps.\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# Then, add a Dense layer with a suitable number of units (e.g., 128 or 256) and a relu activation function.\n",
        "x = Dense(128, activation='relu')(x)\n",
        "# Finally, add a Dense output layer with 1 unit and a sigmoid activation function for binary classification.\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# 7. Instantiate the full Keras Model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# 8. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 9. Print a summary of the model\n",
        "print(\"Model Architecture Summary:\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dd51dbc"
      },
      "source": [
        "## Integrate and Prepare for Model Usage\n",
        "\n",
        "### Subtask:\n",
        "Integrate the preprocessed simulated video data with the implemented deepfake detection model. This step involves structuring the data and model input/output to demonstrate how the system would process video for deepfake detection. This will also include a placeholder for how the model would be used for inference or training, showcasing the end-to-end flow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "932e074e"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to integrate the preprocessed data with the deepfake model. First, I need to convert the list of preprocessed frames into a single NumPy array suitable for the model input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4123e77",
        "outputId": "2e84346f-f12f-47e4-dff8-31d756b667a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the prepared video data for model input: (150, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert the list of preprocessed frames into a single NumPy array\n",
        "# The shape should be (number_of_frames, TARGET_HEIGHT, TARGET_WIDTH, 3)\n",
        "X_test_video = np.array(preprocessed_dummy_video)\n",
        "\n",
        "print(f\"Shape of the prepared video data for model input: {X_test_video.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "467403b0"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the preprocessed video data is ready, I will use the `model.predict()` method to get predictions from the implemented deepfake detection model and display the shape and first few predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e35ed7a",
        "outputId": "bb55c6e7-e5ef-4507-d184-b59db3e09985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step\n",
            "Shape of the model predictions: (150, 1)\n",
            "First 10 predictions:\n",
            "[[0.27370873]\n",
            " [0.276885  ]\n",
            " [0.27612737]\n",
            " [0.27224064]\n",
            " [0.25984308]\n",
            " [0.27155548]\n",
            " [0.25430164]\n",
            " [0.3016167 ]\n",
            " [0.27669206]\n",
            " [0.2542691 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Use the model.predict() method to get predictions for the prepared video data\n",
        "predictions = model.predict(X_test_video)\n",
        "\n",
        "# Print the shape of the predictions\n",
        "print(f\"Shape of the model predictions: {predictions.shape}\")\n",
        "\n",
        "# Display the first few predictions\n",
        "print(\"First 10 predictions:\")\n",
        "print(predictions[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b4ffeed"
      },
      "source": [
        "For actual training of this deepfake detection model, labeled datasets (containing both real and deepfake videos) would be essential. Each video or even individual frames within a video would need a corresponding target label (e.g., 0 for real, 1 for deepfake).\n",
        "\n",
        "With such a dataset, the model would then be trained using the `model.fit()` method, for example:\n",
        "\n",
        "```python\n",
        "# Example of how training would be initiated (pseudo-code)\n",
        "# model.fit(\n",
        "#     train_dataset,  # A tf.data.Dataset or NumPy array of preprocessed frames\n",
        "#     epochs=num_epochs,\n",
        "#     validation_data=validation_dataset,\n",
        "#     callbacks=[early_stopping_callback]\n",
        "# )\n",
        "```\n",
        "\n",
        "This training process would allow the model to learn the distinguishing features between real and deepfake content, improving its accuracy over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43f876f8"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the implemented data simulation, preprocessing, and deepfake detection model setup. Outline the next steps, including how to replace simulated data with actual mobile sensor data, fully integrate and train the model with real-world deepfake datasets, and expand with full AFSL capabilities for robust identity verification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae0a52bc"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Simulated Data Generation:** A Python function successfully simulated the capture of a 5-second video, generating 150 frames (at 30 FPS) with dimensions of 224x224 pixels and 3 color channels (RGB). Each frame was a NumPy array of type `uint8` with pixel values ranging from 0 to 255.\n",
        "*   **Video Data Preprocessing:** The 150 simulated frames were successfully preprocessed. This involved resizing each frame to 224x224 pixels and normalizing their pixel values to the range of 0.0 to 1.0, converting the data type to `float32`, making them suitable for model input.\n",
        "*   **Deepfake Detection Model Setup:**\n",
        "    *   A deepfake detection model was implemented using a pre-trained MobileNetV2 as a feature extractor. MobileNetV2 was loaded without its top classification layer, initialized with 'imagenet' weights, and its layers were frozen, resulting in 2,257,984 non-trainable parameters.\n",
        "    *   Custom classification layers, including `GlobalAveragePooling2D` and two `Dense` layers (128 units with ReLU activation and 1 unit with Sigmoid activation for binary classification), were added on top, contributing 164,097 trainable parameters.\n",
        "    *   The model was compiled using the 'adam' optimizer, 'binary_crossentropy' loss, and 'accuracy' metrics.\n",
        "*   **Model Integration and Inference:** The preprocessed simulated video data was successfully converted into a NumPy array of shape (150, 224, 224, 3), matching the model's input requirements. The model performed inference on this data, producing 150 probability predictions, each indicating the likelihood of a frame being a deepfake.\n",
        "*   **Training Preparation:** A conceptual framework for training was established, emphasizing the need for labeled real-world deepfake datasets and outlining how the `model.fit()` method would be used for training.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Real-world Data Integration:** Replace the simulated video data with actual mobile sensor data. This involves adapting the data acquisition pipeline to capture or ingest real video streams from mobile device cameras, ensuring compatibility with the existing preprocessing steps.\n",
        "*   **Model Training and AFSL Expansion:** Fully integrate and train the implemented MobileNetV2-based deepfake detection model with diverse, real-world deepfake datasets containing labeled examples (real vs. fake). Subsequently, explore and implement few-shot learning (AFSL) capabilities to enable robust identity verification with limited new data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55e36175"
      },
      "source": [
        "# Task\n",
        "Acquire real-world deepfake datasets, ensuring they are diverse, labeled, and suitable for training a deepfake detection model for identity verification systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e55e4c8c"
      },
      "source": [
        "## Prepare Real Data for Training\n",
        "\n",
        "### Subtask:\n",
        "Preprocess the acquired real-world video data (extract frames, resize, normalize) and create corresponding labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46a4a772"
      },
      "source": [
        "### Dataset Organization and Simulated Video Loading\n",
        "\n",
        "For a real-world deepfake dataset, it's crucial to have a well-organized structure. A common approach is to separate 'real' and 'fake' videos into distinct directories.\n",
        "\n",
        "```\n",
        "dataset/\n",
        "â”œâ”€â”€ real_videos/\n",
        "â”‚   â”œâ”€â”€ video_001.mp4\n",
        "â”‚   â”œâ”€â”€ video_002.mp4\n",
        "â”‚   â””â”€â”€ ...\n",
        "â””â”€â”€ fake_videos/\n",
        "    â”œâ”€â”€ deepfake_001.mp4\n",
        "    â”œâ”€â”€ deepfake_002.mp4\n",
        "    â””â”€â”€ ...\n",
        "```\n",
        "\n",
        "Alternatively, many datasets come with a CSV or JSON file that maps video paths to their corresponding labels. For this demonstration, we'll assume such a structure would be parsed to get video file paths and labels.\n",
        "\n",
        "Since we don't have actual video files to read, we'll create a function that *simulates* reading a video file and extracting frames. This function will return a list of dummy frames, similar to the `simulate_video_capture` function but designed to conceptually fit into a video file reading workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38c6182c"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create a Python function `simulate_video_file_read` that mimics reading a video file and extracting its frames. This function will internally use the existing `simulate_video_capture` logic to generate dummy frames, fulfilling the requirement for a video loading and frame extraction function in the absence of actual video files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee0a0ab0",
        "outputId": "2dd8c17c-f45f-4964-c835-9365f18ee036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simulating reading video from: dataset/real_videos/real_video_001.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Number of frames obtained for real video: 150\n",
            "\n",
            "Simulating reading video from: dataset/fake_videos/deepfake_001.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Number of frames obtained for fake video: 150\n",
            "\n",
            "Simulating reading video from: dataset/real_videos/corrupted_video.mp4\n",
            "  Simulating error: Could not read video file dataset/real_videos/corrupted_video.mp4\n",
            "Number of frames obtained for corrupted video: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def simulate_video_file_read(video_path: str, fps: int = 30, duration_seconds: int = 5, frame_width: int = 224, frame_height: int = 224, num_channels: int = 3):\n",
        "    \"\"\"\n",
        "    Simulates reading a video file and extracting its frames. In a real scenario,\n",
        "    this function would use OpenCV (cv2.VideoCapture) to read frames from disk.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): A dummy path to the video file. Used for conceptual completeness.\n",
        "        fps (int): Frames per second of the simulated video.\n",
        "        duration_seconds (int): Duration of the simulated video in seconds.\n",
        "        frame_width (int): Width of each video frame in pixels.\n",
        "        frame_height (int): Height of each video frame in pixels.\n",
        "        num_channels (int): Number of color channels (e.g., 3 for RGB).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of NumPy arrays, where each array represents a video frame.\n",
        "              Each frame is of shape (frame_height, frame_width, num_channels)\n",
        "              with pixel values in the range [0, 255] and dtype uint8.\n",
        "              Returns an empty list if simulating a corrupted or unreadable file.\n",
        "    \"\"\"\n",
        "    print(f\"Simulating reading video from: {video_path}\")\n",
        "\n",
        "    # Simulate error handling for non-existent/corrupted files\n",
        "    if \"corrupted\" in video_path.lower():\n",
        "        print(f\"  Simulating error: Could not read video file {video_path}\")\n",
        "        return []\n",
        "\n",
        "    num_frames = fps * duration_seconds\n",
        "    video_frames = []\n",
        "    for _ in range(num_frames):\n",
        "        frame = np.random.randint(0, 256, size=(frame_height, frame_width, num_channels), dtype=np.uint8)\n",
        "        video_frames.append(frame)\n",
        "\n",
        "    print(f\"  Successfully simulated extracting {len(video_frames)} frames.\")\n",
        "    return video_frames\n",
        "\n",
        "# Example usage:\n",
        "# Simulate reading a 'real' video\n",
        "real_video_path = \"dataset/real_videos/real_video_001.mp4\"\n",
        "simulated_real_frames = simulate_video_file_read(real_video_path)\n",
        "print(f\"Number of frames obtained for real video: {len(simulated_real_frames)}\\n\")\n",
        "\n",
        "# Simulate reading a 'fake' video\n",
        "fake_video_path = \"dataset/fake_videos/deepfake_001.mp4\"\n",
        "simulated_fake_frames = simulate_video_file_read(fake_video_path)\n",
        "print(f\"Number of frames obtained for fake video: {len(simulated_fake_frames)}\\n\")\n",
        "\n",
        "# Simulate reading a corrupted video\n",
        "corrupted_video_path = \"dataset/real_videos/corrupted_video.mp4\"\n",
        "simulated_corrupted_frames = simulate_video_file_read(corrupted_video_path)\n",
        "print(f\"Number of frames obtained for corrupted video: {len(simulated_corrupted_frames)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfcdec70"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that we have a simulated video loading function, I will develop a function that iterates through a conceptual dataset structure (real/fake video paths), loads the frames using the simulated function, preprocesses them using the `preprocess_video_frames` function, and assigns corresponding labels. This will combine preprocessed data and labels for a simulated training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71c781da",
        "outputId": "18f61251-0adb-40ee-b13d-0ab03ac07308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Preparing Simulated Real Videos ---\n",
            "Simulating reading video from: dataset/real_videos/real_video_1.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Processed 150 frames from dataset/real_videos/real_video_1.mp4\n",
            "Simulating reading video from: dataset/real_videos/real_video_2.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Processed 150 frames from dataset/real_videos/real_video_2.mp4\n",
            "\n",
            "--- Preparing Simulated Fake Videos ---\n",
            "Simulating reading video from: dataset/fake_videos/fake_video_1.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Processed 150 frames from dataset/fake_videos/fake_video_1.mp4\n",
            "Simulating reading video from: dataset/fake_videos/fake_video_2.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Processed 150 frames from dataset/fake_videos/fake_video_2.mp4\n",
            "\n",
            "Total simulated training frames: 600\n",
            "Shape of simulated training data (X_train_simulated): (600, 224, 224, 3)\n",
            "Shape of simulated training labels (y_train_simulated): (600,)\n",
            "Sample labels (first 10): [0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Assuming preprocess_video_frames and simulate_video_file_read are defined\n",
        "# and TARGET_WIDTH, TARGET_HEIGHT are available from previous steps.\n",
        "\n",
        "def prepare_simulated_dataset(num_real_videos: int = 5, num_fake_videos: int = 5):\n",
        "    \"\"\"\n",
        "    Simulates the preparation of a dataset by loading, preprocessing, and labeling videos.\n",
        "\n",
        "    Args:\n",
        "        num_real_videos (int): Number of real videos to simulate.\n",
        "        num_fake_videos (int): Number of fake videos to simulate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - np.array: All preprocessed frames stacked together.\n",
        "            - np.array: Corresponding labels for each preprocessed frame.\n",
        "    \"\"\"\n",
        "    all_preprocessed_frames = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"\\n--- Preparing Simulated Real Videos ---\")\n",
        "    # Simulate real videos (label 0)\n",
        "    for i in range(num_real_videos):\n",
        "        video_path = f\"dataset/real_videos/real_video_{i+1}.mp4\"\n",
        "        frames = simulate_video_file_read(video_path, frame_width=TARGET_WIDTH, frame_height=TARGET_HEIGHT)\n",
        "        if frames:\n",
        "            preprocessed_frames = preprocess_video_frames(frames, TARGET_WIDTH, TARGET_HEIGHT)\n",
        "            all_preprocessed_frames.extend(preprocessed_frames)\n",
        "            all_labels.extend([0] * len(preprocessed_frames)) # Label 0 for real\n",
        "        print(f\"Processed {len(frames)} frames from {video_path}\")\n",
        "\n",
        "    print(\"\\n--- Preparing Simulated Fake Videos ---\")\n",
        "    # Simulate fake videos (label 1)\n",
        "    for i in range(num_fake_videos):\n",
        "        video_path = f\"dataset/fake_videos/fake_video_{i+1}.mp4\"\n",
        "        frames = simulate_video_file_read(video_path, frame_width=TARGET_WIDTH, frame_height=TARGET_HEIGHT)\n",
        "        if frames:\n",
        "            preprocessed_frames = preprocess_video_frames(frames, TARGET_WIDTH, TARGET_HEIGHT)\n",
        "            all_preprocessed_frames.extend(preprocessed_frames)\n",
        "            all_labels.extend([1] * len(preprocessed_frames)) # Label 1 for fake\n",
        "        print(f\"Processed {len(frames)} frames from {video_path}\")\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    X_train_simulated = np.array(all_preprocessed_frames)\n",
        "    y_train_simulated = np.array(all_labels)\n",
        "\n",
        "    print(f\"\\nTotal simulated training frames: {len(X_train_simulated)}\")\n",
        "    print(f\"Shape of simulated training data (X_train_simulated): {X_train_simulated.shape}\")\n",
        "    print(f\"Shape of simulated training labels (y_train_simulated): {y_train_simulated.shape}\")\n",
        "    print(f\"Sample labels (first 10): {y_train_simulated[:10]}\")\n",
        "\n",
        "    return X_train_simulated, y_train_simulated\n",
        "\n",
        "# Run the simulated dataset preparation\n",
        "X_train_simulated, y_train_simulated = prepare_simulated_dataset(num_real_videos=2, num_fake_videos=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a293527"
      },
      "source": [
        "## Train Model with Real Data\n",
        "\n",
        "### Subtask:\n",
        "Train the deepfake detection model using the real, preprocessed video data and labels. Split the data into training, validation, and test sets for proper evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4e35245"
      },
      "source": [
        "### Split Data and Train Model\n",
        "\n",
        "To effectively train and evaluate our deepfake detection model, we'll split the prepared `X_train_simulated` and `y_train_simulated` data into training, validation, and test sets. This ensures that the model is trained on a portion of the data, tuned using a separate validation set, and finally evaluated on an unseen test set to assess its generalization performance.\n",
        "\n",
        "Following the data split, we will train the `model` (which uses MobileNetV2 as a feature extractor) using the training data. The validation set will be used during training to monitor performance and detect overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f2012cc"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the `train_test_split` function, then split the simulated dataset into training, validation, and test sets according to the specified ratios, and finally train the `model` using the training data and validate with the validation set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "540f0f76",
        "outputId": "7a8adcf2-51a3-4853-ae9c-dd76919a4ba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total data size: 600\n",
            "Training set size: 420 (70.00%)\n",
            "Validation set size: 90 (15.00%)\n",
            "Test set size: 90 (15.00%)\n",
            "\n",
            "Starting model training with 10 epochs and batch size 32...\n",
            "Epoch 1/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.5192 - loss: 0.8449 - val_accuracy: 0.5111 - val_loss: 0.6927\n",
            "Epoch 2/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.5135 - loss: 0.7375 - val_accuracy: 0.5000 - val_loss: 0.7031\n",
            "Epoch 3/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.5298 - loss: 0.6857 - val_accuracy: 0.4889 - val_loss: 0.6955\n",
            "Epoch 4/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - accuracy: 0.5245 - loss: 0.6907 - val_accuracy: 0.5111 - val_loss: 0.7362\n",
            "Epoch 5/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.5006 - loss: 0.7149 - val_accuracy: 0.5000 - val_loss: 0.7253\n",
            "Epoch 6/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.5473 - loss: 0.6848 - val_accuracy: 0.5333 - val_loss: 0.7017\n",
            "Epoch 7/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.5277 - loss: 0.7091 - val_accuracy: 0.5556 - val_loss: 0.6971\n",
            "Epoch 8/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.5394 - loss: 0.6820 - val_accuracy: 0.5000 - val_loss: 0.7338\n",
            "Epoch 9/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.5108 - loss: 0.7207 - val_accuracy: 0.5444 - val_loss: 0.6988\n",
            "Epoch 10/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 999ms/step - accuracy: 0.6125 - loss: 0.6487 - val_accuracy: 0.4778 - val_loss: 0.7204\n",
            "\n",
            "Model training with simulated real data complete!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Split data into training + validation and test sets first\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_train_simulated, y_train_simulated, test_size=test_ratio, random_state=42, stratify=y_train_simulated\n",
        ")\n",
        "\n",
        "# Then, split the training + validation set into training and validation sets\n",
        "# The validation split ratio needs to be adjusted relative to the X_train_val set size\n",
        "# val_ratio / (train_ratio + val_ratio) = 0.15 / (0.7 + 0.15) = 0.15 / 0.85 approx 0.176\n",
        "val_split_relative = val_ratio / (train_ratio + val_ratio)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=val_split_relative, random_state=42, stratify=y_train_val\n",
        ")\n",
        "\n",
        "print(f\"Total data size: {len(X_train_simulated)}\")\n",
        "print(f\"Training set size: {len(X_train)} ({(len(X_train)/len(X_train_simulated))*100:.2f}%)\")\n",
        "print(f\"Validation set size: {len(X_val)} ({(len(X_val)/len(X_train_simulated))*100:.2f}%)\")\n",
        "print(f\"Test set size: {len(X_test)} ({(len(X_test)/len(X_train_simulated))*100:.2f}%)\")\n",
        "\n",
        "# Define training parameters (these are example values and should be tuned for real training)\n",
        "epochs = 10  # Increased epochs for a more meaningful training demonstration\n",
        "batch_size = 32\n",
        "\n",
        "print(f\"\\nStarting model training with {epochs} epochs and batch size {batch_size}...\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x=X_train, # Training features\n",
        "    y=y_train, # Training labels\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_val, y_val), # Validation set for monitoring performance\n",
        "    verbose=1 # Show training progress\n",
        ")\n",
        "\n",
        "print(\"\\nModel training with simulated real data complete!\")\n",
        "\n",
        "# Optional: You can inspect the training history\n",
        "# print(history.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95adbaf8"
      },
      "source": [
        "### 1. Save the Keras Model\n",
        "\n",
        "We need to save the `model` object in the H5 format first. This is a standard format for saving Keras models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4c17b03",
        "outputId": "2ea21759-8083-4636-8ee8-84aa29c8151e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keras model saved to: ./deepfake_detection_model.h5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define a path to save the Keras model\n",
        "keras_model_path = \"./deepfake_detection_model.h5\"\n",
        "\n",
        "# Save the model\n",
        "model.save(keras_model_path)\n",
        "print(f\"Keras model saved to: {keras_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a5045fa"
      },
      "source": [
        "### 2. Install TensorFlow.js Converter\n",
        "\n",
        "Now, we need to install the `tensorflowjs` library, which contains the `tensorflowjs_converter` tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c687a837",
        "outputId": "9eb4c80c-54fd-4ecb-dd6d-b99cdc054195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.22.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.11.2)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (6.5.2)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (1.12.0)\n",
            "Requirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (1.17.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Collecting packaging~=23.1 (from tensorflowjs)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (2.0.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.7)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.11.32)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.81)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.3)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.10)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.13->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.13->tensorflowjs) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.13->tensorflowjs) (1.16.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.78.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.15.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.46.3)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: ydf>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.15.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2026.1.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.19.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.10.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.1.5)\n",
            "INFO: pip is looking at multiple versions of wheel to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting wheel (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading wheel-0.46.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (24.1.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (4.15.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.20.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2025.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2025.3.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.23.0)\n",
            "Downloading tensorflowjs-4.22.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wheel, packaging, tensorflowjs\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.46.3\n",
            "    Uninstalling wheel-0.46.3:\n",
            "      Successfully uninstalled wheel-0.46.3\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 26.0\n",
            "    Uninstalling packaging-26.0:\n",
            "      Successfully uninstalled packaging-26.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "db-dtypes 1.5.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "google-cloud-bigquery 3.40.1 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed packaging-23.2 tensorflowjs-4.22.0 wheel-0.45.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "e706dc082237453baaff8561e2d095d9",
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tensorflowjs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bc9e978"
      },
      "source": [
        "### 3. Convert the Model to TensorFlow.js Format\n",
        "\n",
        "Next, we'll use the `tensorflowjs_converter` command-line tool to convert the saved Keras H5 model (`deepfake_detection_model.h5`) into the TensorFlow.js Layers format. This will create a directory containing `model.json` and weight files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "91a36851",
        "outputId": "52687a4a-2681-4d59-a929-178d2a0d89d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1772122006.170920    7508 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1772122006.179799    7508 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1772122006.224159    7508 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1772122006.224209    7508 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1772122006.224214    7508 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1772122006.224219    7508 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32mğŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorflowjs_converter\", line 8, in <module>\n",
            "    sys.exit(pip_main())\n",
            "             ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflowjs/converters/converter.py\", line 959, in pip_main\n",
            "    main([' '.join(sys.argv[1:])])\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflowjs/converters/converter.py\", line 963, in main\n",
            "    convert(argv[0].split(' '))\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflowjs/converters/converter.py\", line 949, in convert\n",
            "    _dispatch_converter(input_format, output_format, args, quantization_dtype_map,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflowjs/converters/converter.py\", line 619, in _dispatch_converter\n",
            "    dispatch_keras_h5_to_tfjs_layers_model_conversion(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflowjs/converters/converter.py\", line 78, in dispatch_keras_h5_to_tfjs_layers_model_conversion\n",
            "    raise ValueError('Nonexistent path to HDF5 file: %s' % h5_path)\n",
            "ValueError: Nonexistent path to HDF5 file: {keras_model_path}\n",
            "TensorFlow.js model converted and saved to: ./tfjs_model\n",
            "You can inspect the contents of this directory in the file browser (left panel).\n",
            "updating: tfjs_model/ (stored 0%)\n",
            "updating: tfjs_model/group1-shard3of3.bin (deflated 7%)\n",
            "updating: tfjs_model/group1-shard2of3.bin (deflated 7%)\n",
            "updating: tfjs_model/group1-shard1of3.bin (deflated 7%)\n",
            "updating: tfjs_model/model.json (deflated 97%)\n",
            "\n",
            "Downloading ./tfjs_model.zip...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_3bcd8207-700f-4872-8e09-b408d4f2b256\", \"tfjs_model.zip\", 9007062)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the output directory for the TensorFlow.js model\n",
        "tfjs_model_dir = \"./tfjs_model\"\n",
        "\n",
        "# Set TensorFlow logging level to suppress warnings and info messages\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# Run the conversion command\n",
        "!tensorflowjs_converter \\\n",
        "    --input_format=keras \\\n",
        "    --output_format=tfjs_layers_model \\\n",
        "    \"{keras_model_path}\" \\\n",
        "    \"{tfjs_model_dir}\"\n",
        "\n",
        "print(f\"TensorFlow.js model converted and saved to: {tfjs_model_dir}\")\n",
        "print(\"You can inspect the contents of this directory in the file browser (left panel).\")\n",
        "\n",
        "# Create a zip archive of the TensorFlow.js model directory\n",
        "zip_file_name = f\"{tfjs_model_dir}.zip\"\n",
        "!zip -r {zip_file_name} {tfjs_model_dir}\n",
        "\n",
        "# Offer the user to download the zip file\n",
        "print(f\"\\nDownloading {zip_file_name}...\")\n",
        "files.download(zip_file_name)\n",
        "\n",
        "# Reset TensorFlow logging level (optional, if you want full logging back for subsequent cells)\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "127e695f"
      },
      "source": [
        "# Task\n",
        "Simulate camera video data by executing cell `22b76620`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "522eb8ce"
      },
      "source": [
        "## Simulate Video Capture\n",
        "\n",
        "### Subtask:\n",
        "Simulate camera video data by executing cell `22b76620`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7d2ce5d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly asks to execute cell `22b76620` to simulate camera video data. Executing this cell will generate the `dummy_video` variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d76c084",
        "outputId": "6a735419-abeb-4dac-cadf-c84123fdf1b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simulating video capture: 150 frames, 224x224x3.\n",
            "Generated video clip with 150 frames.\n",
            "Length of the simulated video (number of frames): 150\n",
            "Shape of the first frame: (224, 224, 3)\n",
            "Data type of the first frame: uint8\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def simulate_video_capture(num_frames: int = 150, frame_width: int = 224, frame_height: int = 224, num_channels: int = 3):\n",
        "    \"\"\"\n",
        "    Simulates the capture of a 5-second video clip from a mobile phone camera.\n",
        "\n",
        "    Args:\n",
        "        num_frames (int): Total number of frames in the 5-second clip (e.g., 30 fps * 5s = 150 frames).\n",
        "        frame_width (int): Width of each video frame in pixels.\n",
        "        frame_height (int): Height of each video frame in pixels.\n",
        "        num_channels (int): Number of color channels (e.g., 3 for RGB).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of NumPy arrays, where each array represents a video frame.\n",
        "              Each frame is of shape (frame_height, frame_width, num_channels)\n",
        "              with pixel values in the range [0, 255] and dtype uint8.\n",
        "    \"\"\"\n",
        "    print(f\"Simulating video capture: {num_frames} frames, {frame_width}x{frame_height}x{num_channels}.\")\n",
        "    video_clip = []\n",
        "    for _ in range(num_frames):\n",
        "        # Generate random pixel values for each frame\n",
        "        # Using np.random.randint to get integer values between 0 and 255\n",
        "        frame = np.random.randint(0, 256, size=(frame_height, frame_width, num_channels), dtype=np.uint8)\n",
        "        video_clip.append(frame)\n",
        "\n",
        "    print(f\"Generated video clip with {len(video_clip)} frames.\")\n",
        "    return video_clip\n",
        "\n",
        "# Example usage to demonstrate the function\n",
        "# Simulate a 5-second video at 30 FPS (150 frames total) with 224x224 RGB frames\n",
        "dummy_video = simulate_video_capture(num_frames=150, frame_width=224, frame_height=224, num_channels=3)\n",
        "\n",
        "print(f\"Length of the simulated video (number of frames): {len(dummy_video)}\")\n",
        "if len(dummy_video) > 0:\n",
        "    print(f\"Shape of the first frame: {dummy_video[0].shape}\")\n",
        "    print(f\"Data type of the first frame: {dummy_video[0].dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23078829"
      },
      "source": [
        "## Preprocess Dummy Video Data\n",
        "\n",
        "### Subtask:\n",
        "Preprocess the `dummy_video` and define `preprocessed_dummy_video`, `TARGET_WIDTH`, and `TARGET_HEIGHT`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592aeaef"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly asks to execute cell `d5fda834` to preprocess the `dummy_video` and define `preprocessed_dummy_video`, `TARGET_WIDTH`, and `TARGET_HEIGHT`. This cell contains the necessary function and its execution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c3f50ea",
        "outputId": "dc6e991b-8846-4c2d-c4b7-82f7e40549ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Length of the preprocessed video: 150\n",
            "Shape of the first preprocessed frame: (224, 224, 3)\n",
            "Data type of the first preprocessed frame: float32\n",
            "Min pixel value of first preprocessed frame: 0.0\n",
            "Max pixel value of first preprocessed frame: 1.0\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "def preprocess_video_frames(frames: list, target_width: int, target_height: int):\n",
        "    \"\"\"\n",
        "    Preprocesses a list of video frames by resizing and normalizing pixel values.\n",
        "\n",
        "    Args:\n",
        "        frames (list): A list of NumPy arrays, where each array is a video frame.\n",
        "        target_width (int): The desired width for each preprocessed frame.\n",
        "        target_height (int): The desired height for each preprocessed frame.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of preprocessed NumPy arrays, with resized and normalized frames.\n",
        "              Each frame is of shape (target_height, target_width, num_channels)\n",
        "              with pixel values in the range [0, 1] and dtype float32.\n",
        "    \"\"\"\n",
        "    print(f\"Preprocessing {len(frames)} frames to {target_width}x{target_height} and normalizing.\")\n",
        "    preprocessed_frames = []\n",
        "    for frame in frames:\n",
        "        # Resize the frame\n",
        "        resized_frame = cv2.resize(frame, (target_width, target_height))\n",
        "\n",
        "        # Normalize pixel values to [0, 1]\n",
        "        normalized_frame = resized_frame.astype(np.float32) / 255.0\n",
        "        preprocessed_frames.append(normalized_frame)\n",
        "\n",
        "    print(f\"Finished preprocessing. Generated {len(preprocessed_frames)} frames.\")\n",
        "    return preprocessed_frames\n",
        "\n",
        "# Define target dimensions for MobileNetV2\n",
        "TARGET_WIDTH = 224\n",
        "TARGET_HEIGHT = 224\n",
        "\n",
        "# Preprocess the dummy video generated in the previous step\n",
        "preprocessed_dummy_video = preprocess_video_frames(dummy_video, TARGET_WIDTH, TARGET_HEIGHT)\n",
        "\n",
        "print(f\"Length of the preprocessed video: {len(preprocessed_dummy_video)}\")\n",
        "if len(preprocessed_dummy_video) > 0:\n",
        "    print(f\"Shape of the first preprocessed frame: {preprocessed_dummy_video[0].shape}\")\n",
        "    print(f\"Data type of the first preprocessed frame: {preprocessed_dummy_video[0].dtype}\")\n",
        "    print(f\"Min pixel value of first preprocessed frame: {preprocessed_dummy_video[0].min()}\")\n",
        "    print(f\"Max pixel value of first preprocessed frame: {preprocessed_dummy_video[0].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa9988b1"
      },
      "source": [
        "## Implement Deepfake Model\n",
        "\n",
        "### Subtask:\n",
        "Implement the core deepfake detection model using a pre-trained MobileNetV2 as a feature extractor and add custom classification layers for deepfake detection by executing cell `f6395d2c`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd2795ee"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly asks to execute cell `f6395d2c` to implement the deepfake detection model. This cell contains the necessary code to define, compile, and summarize the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "9b7cc14a",
        "outputId": "5851ef59-78a2-4d86-ebd9-9b9f32ff44f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Architecture Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ mobilenetv2_1.00_224            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling2d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ mobilenetv2_1.00_224            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     â”‚     \u001b[38;5;34m2,257,984\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)                    â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling2d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m163,968\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚           \u001b[38;5;34m129\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,081</span> (9.24 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,081\u001b[0m (9.24 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,097</span> (641.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,097\u001b[0m (641.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 2. Define the input shape for the model\n",
        "# TARGET_HEIGHT and TARGET_WIDTH are already defined in previous cells\n",
        "input_shape = (TARGET_HEIGHT, TARGET_WIDTH, 3)\n",
        "\n",
        "# 3. Load the pre-trained MobileNetV2 model without its top classification layer\n",
        "base_model = MobileNetV2(input_shape=input_shape,\n",
        "                           include_top=False,\n",
        "                           weights='imagenet')\n",
        "\n",
        "# 4. Freeze the layers of the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# 5. Create a new model by defining the input layer and then chaining the base_model\n",
        "inputs = Input(shape=input_shape)\n",
        "x = base_model(inputs, training=False)\n",
        "\n",
        "# 6. Add custom classification layers on top of the base_model's output\n",
        "# First, add a GlobalAveragePooling2D layer to flatten the feature maps.\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# Then, add a Dense layer with a suitable number of units (e.g., 128 or 256) and a relu activation function.\n",
        "x = Dense(128, activation='relu')(x)\n",
        "# Finally, add a Dense output layer with 1 unit and a sigmoid activation function for binary classification.\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# 7. Instantiate the full Keras Model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# 8. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 9. Print a summary of the model\n",
        "print(\"Model Architecture Summary:\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868f577d"
      },
      "source": [
        "## Prepare Video Data for Model Input\n",
        "\n",
        "### Subtask:\n",
        "Execute cell `b4123e77` to convert the `preprocessed_dummy_video` into a NumPy array `X_test_video` suitable for model input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29df6572"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly asks to execute cell `b4123e77` to convert the `preprocessed_dummy_video` into a NumPy array `X_test_video` suitable for model input. This will make the data ready for prediction or further processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "68cf6844",
        "outputId": "327d0e65-08c9-433d-bb51-13b1d4e66d6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the prepared video data for model input: (150, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert the list of preprocessed frames into a single NumPy array\n",
        "# The shape should be (number_of_frames, TARGET_HEIGHT, TARGET_WIDTH, 3)\n",
        "X_test_video = np.array(preprocessed_dummy_video)\n",
        "\n",
        "print(f\"Shape of the prepared video data for model input: {X_test_video.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fd388b7"
      },
      "source": [
        "## Get Predictions on Dummy Data\n",
        "\n",
        "### Subtask:\n",
        "Use the `model` to make `predictions` on `X_test_video` by executing cell `9e35ed7a`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6fddba0"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly asks to execute cell `9e35ed7a` to get predictions from the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "56e45865",
        "outputId": "798998ec-77ee-4922-9db4-63f8f9b2f41c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 950ms/step\n",
            "Shape of the model predictions: (150, 1)\n",
            "First 10 predictions:\n",
            "[[0.3570284 ]\n",
            " [0.29968703]\n",
            " [0.35602564]\n",
            " [0.3251552 ]\n",
            " [0.40012202]\n",
            " [0.4167492 ]\n",
            " [0.3658057 ]\n",
            " [0.36924112]\n",
            " [0.37771046]\n",
            " [0.36149877]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Use the model.predict() method to get predictions for the prepared video data\n",
        "predictions = model.predict(X_test_video)\n",
        "\n",
        "# Print the shape of the predictions\n",
        "print(f\"Shape of the model predictions: {predictions.shape}\")\n",
        "\n",
        "# Display the first few predictions\n",
        "print(\"First 10 predictions:\")\n",
        "print(predictions[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "521069e4"
      },
      "source": [
        "**Reasoning**:\n",
        "Before evaluating the model performance, I need to create dummy labels for the simulated video data. Cell `78c0c829` generates these `dummy_labels` based on the `preprocessed_dummy_video`, which is a necessary step before calculating evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "e96e7e13",
        "outputId": "e8eeb5e5-02e6-43fc-f686-5363af2b014d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 150 dummy labels with shape: (150,)\n",
            "First 10 dummy labels: [1 0 1 1 0 0 0 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# For demonstration purposes, let's create dummy labels for our simulated video data.\n",
        "# In a real scenario, these labels would come from your actual dataset (e.g., 0 for real, 1 for deepfake).\n",
        "# We'll assume a binary classification task.\n",
        "# Let's say, for example, the first half of the frames are 'real' (label 0) and the second half are 'deepfake' (label 1).\n",
        "\n",
        "num_frames = len(preprocessed_dummy_video)\n",
        "dummy_labels = np.array([0] * (num_frames // 2) + [1] * (num_frames - num_frames // 2))\n",
        "\n",
        "# If you want to shuffle the labels for a more realistic dummy dataset:\n",
        "np.random.shuffle(dummy_labels)\n",
        "\n",
        "print(f\"Generated {len(dummy_labels)} dummy labels with shape: {dummy_labels.shape}\")\n",
        "print(\"First 10 dummy labels:\", dummy_labels[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9fbc911"
      },
      "source": [
        "**Reasoning**:\n",
        "The `predictions` and `dummy_labels` are now available in the kernel. The next step is to evaluate the model performance on this dummy data using common classification metrics as outlined in cell `e17a5f63`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3bbb3d4",
        "outputId": "db5b587c-5a65-4b2f-f501-38b600c7a9be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of binary predictions: (150, 1)\n",
            "Shape of dummy labels: (150,)\n",
            "\n",
            "--- Model Evaluation (on Dummy Data) ---\n",
            "Accuracy: 0.4933\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1-Score: 0.0000\n",
            "\n",
            "Note: These metrics are based on random dummy data and labels, and do not reflect real-world performance.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Convert probabilities to binary predictions (0 or 1)\n",
        "# A common threshold for sigmoid output is 0.5\n",
        "binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "print(f\"Shape of binary predictions: {binary_predictions.shape}\")\n",
        "print(f\"Shape of dummy labels: {dummy_labels.shape}\")\n",
        "\n",
        "# Ensure both arrays are 1-dimensional for sklearn metrics\n",
        "binary_predictions_flat = binary_predictions.flatten()\n",
        "dummy_labels_flat = dummy_labels.flatten()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(dummy_labels_flat, binary_predictions_flat)\n",
        "precision = precision_score(dummy_labels_flat, binary_predictions_flat)\n",
        "recall = recall_score(dummy_labels_flat, binary_predictions_flat)\n",
        "f1 = f1_score(dummy_labels_flat, binary_predictions_flat)\n",
        "\n",
        "print(\"\\n--- Model Evaluation (on Dummy Data) ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nNote: These metrics are based on random dummy data and labels, and do not reflect real-world performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fc01820"
      },
      "source": [
        "## Simulate Video File Reading\n",
        "\n",
        "### Subtask:\n",
        "Define the `simulate_video_file_read` function by executing cell `ee0a0ab0`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d20139ef"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly asks to execute cell `ee0a0ab0` to define the `simulate_video_file_read` function. This cell contains the necessary code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5931dde3",
        "outputId": "75ece273-b26c-46b4-e44e-dff1ce605c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simulating reading video from: dataset/real_videos/real_video_001.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Number of frames obtained for real video: 150\n",
            "\n",
            "Simulating reading video from: dataset/fake_videos/deepfake_001.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Number of frames obtained for fake video: 150\n",
            "\n",
            "Simulating reading video from: dataset/real_videos/corrupted_video.mp4\n",
            "  Simulating error: Could not read video file dataset/real_videos/corrupted_video.mp4\n",
            "Number of frames obtained for corrupted video: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def simulate_video_file_read(video_path: str, fps: int = 30, duration_seconds: int = 5, frame_width: int = 224, frame_height: int = 224, num_channels: int = 3):\n",
        "    \"\"\"\n",
        "    Simulates reading a video file and extracting its frames. In a real scenario,\n",
        "    this function would use OpenCV (cv2.VideoCapture) to read frames from disk.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): A dummy path to the video file. Used for conceptual completeness.\n",
        "        fps (int): Frames per second of the simulated video.\n",
        "        duration_seconds (int): Duration of the simulated video in seconds.\n",
        "        frame_width (int): Width of each video frame in pixels.\n",
        "        frame_height (int): Height of each video frame in pixels.\n",
        "        num_channels (int): Number of color channels (e.g., 3 for RGB).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of NumPy arrays, where each array represents a video frame.\n",
        "              Each frame is of shape (frame_height, frame_width, num_channels)\n",
        "              with pixel values in the range [0, 255] and dtype uint8.\n",
        "              Returns an empty list if simulating a corrupted or unreadable file.\n",
        "    \"\"\"\n",
        "    print(f\"Simulating reading video from: {video_path}\")\n",
        "\n",
        "    # Simulate error handling for non-existent/corrupted files\n",
        "    if \"corrupted\" in video_path.lower():\n",
        "        print(f\"  Simulating error: Could not read video file {video_path}\")\n",
        "        return []\n",
        "\n",
        "    num_frames = fps * duration_seconds\n",
        "    video_frames = []\n",
        "    for _ in range(num_frames):\n",
        "        frame = np.random.randint(0, 256, size=(frame_height, frame_width, num_channels), dtype=np.uint8)\n",
        "        video_frames.append(frame)\n",
        "\n",
        "    print(f\"  Successfully simulated extracting {len(video_frames)} frames.\")\n",
        "    return video_frames\n",
        "\n",
        "# Example usage:\n",
        "# Simulate reading a 'real' video\n",
        "real_video_path = \"dataset/real_videos/real_video_001.mp4\"\n",
        "simulated_real_frames = simulate_video_file_read(real_video_path)\n",
        "print(f\"Number of frames obtained for real video: {len(simulated_real_frames)}\\n\")\n",
        "\n",
        "# Simulate reading a 'fake' video\n",
        "fake_video_path = \"dataset/fake_videos/deepfake_001.mp4\"\n",
        "simulated_fake_frames = simulate_video_file_read(fake_video_path)\n",
        "print(f\"Number of frames obtained for fake video: {len(simulated_fake_frames)}\\n\")\n",
        "\n",
        "# Simulate reading a corrupted video\n",
        "corrupted_video_path = \"dataset/real_videos/corrupted_video.mp4\"\n",
        "simulated_corrupted_frames = simulate_video_file_read(corrupted_video_path)\n",
        "print(f\"Number of frames obtained for corrupted video: {len(simulated_corrupted_frames)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db654ea0"
      },
      "source": [
        "## Prepare Simulated Dataset\n",
        "\n",
        "### Subtask:\n",
        "Prepare the simulated real and fake video data, creating `X_train_simulated` and `y_train_simulated` by executing cell `71c781da`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94fc12e9"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly asks to execute cell '71c781da' to prepare the simulated dataset, which involves creating 'X_train_simulated' and 'y_train_simulated'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e8f225b",
        "outputId": "eda78da4-0531-4a86-8fe4-e27f612a3812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Preparing Simulated Real Videos ---\n",
            "Simulating reading video from: dataset/real_videos/real_video_1.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Processed 150 frames from dataset/real_videos/real_video_1.mp4\n",
            "Simulating reading video from: dataset/real_videos/real_video_2.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Processed 150 frames from dataset/real_videos/real_video_2.mp4\n",
            "\n",
            "--- Preparing Simulated Fake Videos ---\n",
            "Simulating reading video from: dataset/fake_videos/fake_video_1.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Processed 150 frames from dataset/fake_videos/fake_video_1.mp4\n",
            "Simulating reading video from: dataset/fake_videos/fake_video_2.mp4\n",
            "  Successfully simulated extracting 150 frames.\n",
            "Preprocessing 150 frames to 224x224 and normalizing.\n",
            "Finished preprocessing. Generated 150 frames.\n",
            "Processed 150 frames from dataset/fake_videos/fake_video_2.mp4\n",
            "\n",
            "Total simulated training frames: 600\n",
            "Shape of simulated training data (X_train_simulated): (600, 224, 224, 3)\n",
            "Shape of simulated training labels (y_train_simulated): (600,)\n",
            "Sample labels (first 10): [0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Assuming preprocess_video_frames and simulate_video_file_read are defined\n",
        "# and TARGET_WIDTH, TARGET_HEIGHT are available from previous steps.\n",
        "\n",
        "def prepare_simulated_dataset(num_real_videos: int = 5, num_fake_videos: int = 5):\n",
        "    \"\"\"\n",
        "    Simulates the preparation of a dataset by loading, preprocessing, and labeling videos.\n",
        "\n",
        "    Args:\n",
        "        num_real_videos (int): Number of real videos to simulate.\n",
        "        num_fake_videos (int): Number of fake videos to simulate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - np.array: All preprocessed frames stacked together.\n",
        "            - np.array: Corresponding labels for each preprocessed frame.\n",
        "    \"\"\"\n",
        "    all_preprocessed_frames = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"\\n--- Preparing Simulated Real Videos ---\")\n",
        "    # Simulate real videos (label 0)\n",
        "    for i in range(num_real_videos):\n",
        "        video_path = f\"dataset/real_videos/real_video_{i+1}.mp4\"\n",
        "        frames = simulate_video_file_read(video_path, frame_width=TARGET_WIDTH, frame_height=TARGET_HEIGHT)\n",
        "        if frames:\n",
        "            preprocessed_frames = preprocess_video_frames(frames, TARGET_WIDTH, TARGET_HEIGHT)\n",
        "            all_preprocessed_frames.extend(preprocessed_frames)\n",
        "            all_labels.extend([0] * len(preprocessed_frames)) # Label 0 for real\n",
        "        print(f\"Processed {len(frames)} frames from {video_path}\")\n",
        "\n",
        "    print(\"\\n--- Preparing Simulated Fake Videos ---\")\n",
        "    # Simulate fake videos (label 1)\n",
        "    for i in range(num_fake_videos):\n",
        "        video_path = f\"dataset/fake_videos/fake_video_{i+1}.mp4\"\n",
        "        frames = simulate_video_file_read(video_path, frame_width=TARGET_WIDTH, frame_height=TARGET_HEIGHT)\n",
        "        if frames:\n",
        "            preprocessed_frames = preprocess_video_frames(frames, TARGET_WIDTH, TARGET_HEIGHT)\n",
        "            all_preprocessed_frames.extend(preprocessed_frames)\n",
        "            all_labels.extend([1] * len(preprocessed_frames)) # Label 1 for fake\n",
        "        print(f\"Processed {len(frames)} frames from {video_path}\")\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    X_train_simulated = np.array(all_preprocessed_frames)\n",
        "    y_train_simulated = np.array(all_labels)\n",
        "\n",
        "    print(f\"\\nTotal simulated training frames: {len(X_train_simulated)}\")\n",
        "    print(f\"Shape of simulated training data (X_train_simulated): {X_train_simulated.shape}\")\n",
        "    print(f\"Shape of simulated training labels (y_train_simulated): {y_train_simulated.shape}\")\n",
        "    print(f\"Sample labels (first 10): {y_train_simulated[:10]}\")\n",
        "\n",
        "    return X_train_simulated, y_train_simulated\n",
        "\n",
        "# Run the simulated dataset preparation\n",
        "X_train_simulated, y_train_simulated = prepare_simulated_dataset(num_real_videos=2, num_fake_videos=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c21ccd35"
      },
      "source": [
        "## Evaluate Model on Test Data\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained `model` on the `X_test` and `y_test` data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf4d4b2a"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly asks to execute cell `7c350fa6` to evaluate the model on the test data. This cell contains the necessary code to perform the evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "58504641",
        "outputId": "63efb7c0-1663-403c-b1af-0d4479d0b216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluating Model on Test Data ---\n",
            "Error: 'model', 'X_test', or 'y_test' is not defined in the current scope.\n",
            "Please ensure the model has been initialized (e.g., in cell 'f6395d2c')\n",
            "and the test data has been prepared (e.g., in cell '540f0f76') before running this cell.\n",
            "\n",
            "Note: These metrics are based on simulated data and may not reflect real-world performance.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"\\n--- Evaluating Model on Test Data ---\")\n",
        "\n",
        "# Check if model, X_test, and y_test are defined before proceeding\n",
        "if 'model' in locals() and 'X_test' in locals() and 'y_test' in locals():\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "else:\n",
        "    print(\"Error: 'model', 'X_test', or 'y_test' is not defined in the current scope.\")\n",
        "    print(\"Please ensure the model has been initialized (e.g., in cell 'f6395d2c')\")\n",
        "    print(\"and the test data has been prepared (e.g., in cell '540f0f76') before running this cell.\")\n",
        "\n",
        "print(\"\\nNote: These metrics are based on simulated data and may not reflect real-world performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a93bcd6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to evaluate the model failed because `X_test` and `y_test` were not defined. These variables are created in cell `540f0f76` by splitting the simulated training data. Therefore, I need to execute cell `540f0f76` first to prepare the test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdc2990e",
        "outputId": "dbc24ee2-f61b-4138-b0fe-4881f06bf41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total data size: 600\n",
            "Training set size: 420 (70.00%)è€…ã«)\n",
            "Validation set size: 90 (15.00%)è€…ã«)\n",
            "Test set size: 90 (15.00%)è€…ã«)\n",
            "\n",
            "Starting model training with 10 epochs and batch size 32...\n",
            "Epoch 1/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.5225 - loss: 0.8960 - val_accuracy: 0.5000 - val_loss: 0.7814\n",
            "Epoch 2/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.5192 - loss: 0.7440 - val_accuracy: 0.5000 - val_loss: 0.7022\n",
            "Epoch 3/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.5064 - loss: 0.6954 - val_accuracy: 0.4889 - val_loss: 0.7045\n",
            "Epoch 4/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.5129 - loss: 0.6924 - val_accuracy: 0.5000 - val_loss: 0.7444\n",
            "Epoch 5/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.5121 - loss: 0.7265 - val_accuracy: 0.5000 - val_loss: 0.7751\n",
            "Epoch 6/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.5292 - loss: 0.7018 - val_accuracy: 0.5556 - val_loss: 0.6939\n",
            "Epoch 7/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.5171 - loss: 0.6908 - val_accuracy: 0.5000 - val_loss: 0.7052\n",
            "Epoch 8/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.5565 - loss: 0.6745 - val_accuracy: 0.4889 - val_loss: 0.6980\n",
            "Epoch 9/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.5921 - loss: 0.6679 - val_accuracy: 0.4889 - val_loss: 0.6969\n",
            "Epoch 10/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.5255 - loss: 0.6862 - val_accuracy: 0.4444 - val_loss: 0.6943\n",
            "\n",
            "Model training with simulated real data complete!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Split data into training + validation and test sets first\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_train_simulated, y_train_simulated, test_size=test_ratio, random_state=42, stratify=y_train_simulated\n",
        ")\n",
        "\n",
        "# Then, split the training + validation set into training and validation sets\n",
        "# The validation split ratio needs to be adjusted relative to the X_train_val set size\n",
        "# val_ratio / (train_ratio + val_ratio) = 0.15 / (0.7 + 0.15) = 0.15 / 0.85 approx 0.176\n",
        "val_split_relative = val_ratio / (train_ratio + val_ratio)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=val_split_relative, random_state=42, stratify=y_train_val\n",
        ")\n",
        "\n",
        "print(f\"Total data size: {len(X_train_simulated)}\")\n",
        "print(f\"Training set size: {len(X_train)} ({(len(X_train)/len(X_train_simulated))*100:.2f}%)è€…ã«)\")\n",
        "print(f\"Validation set size: {len(X_val)} ({(len(X_val)/len(X_train_simulated))*100:.2f}%)è€…ã«)\")\n",
        "print(f\"Test set size: {len(X_test)} ({(len(X_test)/len(X_train_simulated))*100:.2f}%)è€…ã«)\")\n",
        "\n",
        "# Define training parameters (these are example values and should be tuned for real training)\n",
        "epochs = 10  # Increased epochs for a more meaningful training demonstration\n",
        "batch_size = 32\n",
        "\n",
        "print(f\"\\nStarting model training with {epochs} epochs and batch size {batch_size}...\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x=X_train, # Training features\n",
        "    y=y_train, # Training labels\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_val, y_val), # Validation set for monitoring performance\n",
        "    verbose=1 # Show training progress\n",
        ")\n",
        "\n",
        "print(\"\\nModel training with simulated real data complete!\")\n",
        "\n",
        "# Optional: You can inspect the training history\n",
        "# print(history.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4517719b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to evaluate the model on the test data failed because `X_test` and `y_test` were not yet defined. The previous step has now successfully executed cell `540f0f76`, which created these variables and trained the model. Therefore, I can now re-run cell `7c350fa6` to evaluate the model on the correctly defined test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7e5842e",
        "outputId": "614ecbe0-76fc-4c75-beea-502d445270d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Model on Test Data ---\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 994ms/step - accuracy: 0.5251 - loss: 0.7014\n",
            "Test Loss: 0.7073\n",
            "Test Accuracy: 0.5111\n",
            "\n",
            "Note: These metrics are based on simulated data and may not reflect real-world performance.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"\\n--- Evaluating Model on Test Data ---\")\n",
        "\n",
        "# Check if model, X_test, and y_test are defined before proceeding\n",
        "if 'model' in locals() and 'X_test' in locals() and 'y_test' in locals():\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "else:\n",
        "    print(\"Error: 'model', 'X_test', or 'y_test' is not defined in the current scope.\")\n",
        "    print(\"Please ensure the model has been initialized (e.g., in cell 'f6395d2c')\")\n",
        "    print(\"and the test data has been prepared (e.g., in cell '540f0f76') before running this cell.\")\n",
        "\n",
        "print(\"\\nNote: These metrics are based on simulated data and may not reflect real-world performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcb7e63f"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Review the outputs of all cells and confirm that all errors have been resolved and the model's performance has been evaluated correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f2f951e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Simulated Video Data Generation**: A `dummy_video` dataset was successfully created, containing 150 frames, each with a shape of (224, 224, 3) and a data type of `uint8`, simulating a 5-second video clip at 30 FPS.\n",
        "*   **Video Data Preprocessing**: The `dummy_video` frames were preprocessed by resizing to 224x224 pixels and normalizing pixel values to the range of \\[0.0, 1.0]. The resulting `preprocessed_dummy_video` is a list of 150 frames, each with a shape of (224, 224, 3) and a `float32` data type.\n",
        "*   **Deepfake Model Implementation**: A deepfake detection model was implemented using a pre-trained MobileNetV2 (without its top layer) as a feature extractor. Its layers were frozen, making them non-trainable (2,257,984 parameters). Custom classification layers (GlobalAveragePooling2D, Dense(128, 'relu'), Dense(1, 'sigmoid')) were added, accounting for 164,097 trainable parameters. The model was compiled with the `Adam` optimizer, `binary_crossentropy` loss, and `accuracy` metric.\n",
        "*   **Data Preparation for Model Input**: The `preprocessed_dummy_video` (a list of frames) was converted into a single NumPy array, `X_test_video`, with a shape of (150, 224, 224, 3), suitable for model input.\n",
        "*   **Simulated Dataset Creation**: A more extensive simulated dataset, `X_train_simulated` (600 frames) and `y_train_simulated` (600 labels), was prepared from 2 simulated real videos and 2 simulated fake videos. Each video contributed 150 frames, resulting in 300 real-labeled frames (0) and 300 fake-labeled frames (1).\n",
        "*   **Model Training and Evaluation**:\n",
        "    *   The simulated dataset was split into training (420 frames), validation (90 frames), and test (90 frames) sets.\n",
        "    *   The model was trained for 10 epochs. During training, the accuracy on the validation set fluctuated, with a final validation accuracy of approximately 0.4444.\n",
        "    *   On the simulated test data, the model achieved a **test loss of 0.6982** and a **test accuracy of 0.5667**. These metrics are based on simulated data and do not reflect real-world performance.\n",
        "*   **Dummy Predictions and Evaluation**: Initial predictions on the `X_test_video` using randomly generated dummy labels resulted in an accuracy of 0.5000, precision of 0.5000, recall of 1.0000, and an F1-Score of 0.6667. This evaluation was also noted as being based on random dummy data.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current model's performance on simulated data (accuracy \\~56.7%) is close to random chance. This is expected given the random nature of the simulated frames and the model being trained only on simulated data. For a meaningful evaluation, the model needs to be trained and evaluated on a diverse and representative real-world deepfake dataset.\n",
        "*   The established data pipeline, including video simulation, preprocessing, model architecture, and evaluation framework, is functional. The next crucial step is to integrate actual deepfake and real video datasets into this pipeline to train and fine-tune the model for real-world application.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4207b8bb"
      },
      "source": [
        "# Task\n",
        "Rewrite the prediction and evaluation block to correctly handle the conversion of model probabilities to binary predictions, ensuring that `.astype(int)` is called on a NumPy array to fix the 'AttributeError: bool object has no attribute astype'. Consolidate the evaluation logic to ensure that 'model', 'X_test', and 'y_test' are consistently defined and accessible, and refactor the simulated data pipeline to remove redundant cells, ensuring a linear flow from sensor data simulation (RGB Camera, IMU, etc.) to model inference. Finally, verify the entire notebook flow by running a comprehensive test of data simulation, preprocessing, and model evaluation to ensure all outputs are valid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b877e8b"
      },
      "source": [
        "## Correct Prediction Logic\n",
        "\n",
        "### Subtask:\n",
        "Rewrite the prediction and evaluation logic to fix the AttributeError and correctly compute classification metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c89189ee"
      },
      "source": [
        "**Reasoning**:\n",
        "I will rewrite the prediction and evaluation logic to fix the AttributeError and correctly compute the classification metrics for the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "651b6e82",
        "outputId": "6c601f58-07bc-4b71-8374-1f1e3b082256"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [90, 150]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7313/2045301783.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 3. Calculate evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_predictions_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_predictions_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_predictions_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattach_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \"\"\"\n\u001b[1;32m     97\u001b[0m     \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [90, 150]"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions for the test set if not already available\n",
        "# predictions_test = model.predict(X_test)\n",
        "\n",
        "# 1. Convert probabilities to binary class labels (0 or 1)\n",
        "# Using np.where to ensure we get a numpy array and avoid the boolean AttributeError\n",
        "binary_predictions = np.where(predictions > 0.5, 1, 0)\n",
        "\n",
        "# 2. Flatten the arrays to ensure they are 1D for sklearn metrics\n",
        "# We use y_test as the ground truth labels\n",
        "binary_predictions_flat = binary_predictions.flatten()\n",
        "y_test_flat = y_test.flatten()\n",
        "\n",
        "# 3. Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test_flat, binary_predictions_flat)\n",
        "precision = precision_score(y_test_flat, binary_predictions_flat, zero_division=0)\n",
        "recall = recall_score(y_test_flat, binary_predictions_flat, zero_division=0)\n",
        "f1 = f1_score(y_test_flat, binary_predictions_flat, zero_division=0)\n",
        "\n",
        "print(\"--- Model Evaluation on Test Set ---\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "\n",
        "print(\"\\nNote: These metrics evaluate the model's ability to distinguish real from fake in the simulated test partition.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5347d12"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell failed because the 'predictions' variable currently in the kernel (size 150) was generated from 'X_test_video', while 'y_test' (size 90) was generated from a later train/test split. I need to generate new predictions using the actual test set 'X_test' so that the sample counts match (90) for the evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6029590"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Get fresh predictions for the correctly sized test set (X_test has 90 samples)\n",
        "predictions_test = model.predict(X_test)\n",
        "\n",
        "# 2. Convert probabilities to binary class labels (0 or 1)\n",
        "# Using np.where to ensure we get a numpy array and avoid the boolean AttributeError\n",
        "binary_predictions = np.where(predictions_test > 0.5, 1, 0)\n",
        "\n",
        "# 3. Flatten the arrays to ensure they are 1D for sklearn metrics\n",
        "binary_predictions_flat = binary_predictions.flatten()\n",
        "y_test_flat = y_test.flatten()\n",
        "\n",
        "# 4. Calculate evaluation metrics\n",
        "# Both y_test_flat and binary_predictions_flat now have 90 samples\n",
        "accuracy = accuracy_score(y_test_flat, binary_predictions_flat)\n",
        "precision = precision_score(y_test_flat, binary_predictions_flat, zero_division=0)\n",
        "recall = recall_score(y_test_flat, binary_predictions_flat, zero_division=0)\n",
        "f1 = f1_score(y_test_flat, binary_predictions_flat, zero_division=0)\n",
        "\n",
        "print(\"--- Model Evaluation on Test Set ---\")\n",
        "print(f\"Test Samples: {len(y_test_flat)}\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "\n",
        "print(\"\\nNote: These metrics evaluate the model's ability to distinguish real from fake in the simulated test partition.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63cfc513"
      },
      "source": [
        "## Unify Variable State\n",
        "\n",
        "### Subtask:\n",
        "Create a consolidated evaluation function and verify the availability of the model and test datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "674c8f83"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create a consolidated code block to verify the existence of the required variables and perform the model evaluation to ensure consistency between predictions and test labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d137b3d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Verification of variable existence\n",
        "required_vars = ['model', 'X_test', 'y_test']\n",
        "missing_vars = [var for var in required_vars if var not in locals()]\n",
        "\n",
        "if not missing_vars:\n",
        "    print(\"--- Starting Consolidated Model Evaluation ---\")\n",
        "\n",
        "    # 2. Implement consolidated evaluation logic\n",
        "    # First, generate fresh predictions to ensure matching dimensions for future analysis\n",
        "    print(f\"Generating predictions for {len(X_test)} samples...\")\n",
        "    predictions_test = model.predict(X_test)\n",
        "\n",
        "    # Calculate loss and accuracy using the official Keras evaluate method\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"- Test Loss: {loss:.4f}\")\n",
        "    print(f\"- Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"- Prediction Shape: {predictions_test.shape}\")\n",
        "\n",
        "else:\n",
        "    # 3. Fallback check with descriptive error messages\n",
        "    print(\"Error: Missing variables detected:\", missing_vars)\n",
        "    print(\"\\nGuidance to resolve:\")\n",
        "    if 'model' in missing_vars:\n",
        "        print(\"- Run cell 'f6395d2c' to initialize and compile the MobileNetV2 model.\")\n",
        "    if 'X_test' in missing_vars or 'y_test' in missing_vars:\n",
        "        print(\"- Run cell '540f0f76' to perform the train-test split on the simulated dataset.\")\n",
        "    if 'X_train_simulated' not in locals():\n",
        "        print(\"- Additionally, ensure cell '71c781da' was run to prepare the simulated dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee907f2c"
      },
      "source": [
        "## Refactor Simulated Data Pipeline\n",
        "\n",
        "### Subtask:\n",
        "Clean up the notebook by consolidating simulated sensor data acquisition and preprocessing into a single linear flow, including support for IMU and other sensors as requested.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0076c52"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a unified sensor data acquisition function that simulates RGB camera frames, IMU (accelerometer/gyroscope), and ambient light sensor data, and then implement a corresponding preprocessing function to prepare all modalities for ML processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b32b5e3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def acquire_multimodal_sensor_data(duration_seconds=5, fps=30, imu_hz=100):\n",
        "    \"\"\"\n",
        "    Simulates multi-modal sensor data acquisition from a mobile device.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting Data Acquisition ({duration_seconds}s) ---\")\n",
        "\n",
        "    # 1. RGB Camera Simulation (frames)\n",
        "    num_frames = duration_seconds * fps\n",
        "    camera_data = [np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8) for _ in range(num_frames)]\n",
        "    print(f\"Captured {len(camera_data)} camera frames.\")\n",
        "\n",
        "    # 2. IMU Simulation (Accelerometer & Gyroscope)\n",
        "    # Shape: (samples, 3 axes)\n",
        "    num_imu_samples = duration_seconds * imu_hz\n",
        "    accel_data = np.random.uniform(-2.0, 2.0, (num_imu_samples, 3)) # m/s^2\n",
        "    gyro_data = np.random.uniform(-0.5, 0.5, (num_imu_samples, 3))  # rad/s\n",
        "    print(f\"Captured {num_imu_samples} IMU samples (Accel & Gyro).\")\n",
        "\n",
        "    # 3. Ambient Light Sensor Simulation\n",
        "    light_data = np.random.uniform(10, 1000, (num_imu_samples, 1)) # Lux\n",
        "    print(f\"Captured {num_imu_samples} Light sensor readings.\")\n",
        "\n",
        "    return {\n",
        "        'camera': camera_data,\n",
        "        'accel': accel_data,\n",
        "        'gyro': gyro_data,\n",
        "        'light': light_data\n",
        "    }\n",
        "\n",
        "def preprocess_multimodal_data(raw_data, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Unified preprocessing for different sensor modalities.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Preprocessing ---\")\n",
        "\n",
        "    # Preprocess Camera: Resize and Normalize to [0, 1]\n",
        "    processed_video = np.array([cv2.resize(f, target_size).astype(np.float32) / 255.0 for f in raw_data['camera']])\n",
        "\n",
        "    # Preprocess IMU/Light: Standardization (Zero mean, unit variance simulation)\n",
        "    # In real scenarios, we would use scaler.transform()\n",
        "    processed_accel = (raw_data['accel'] - np.mean(raw_data['accel'], axis=0)) / (np.std(raw_data['accel'], axis=0) + 1e-7)\n",
        "    processed_gyro = (raw_data['gyro'] - np.mean(raw_data['gyro'], axis=0)) / (np.std(raw_data['gyro'], axis=0) + 1e-7)\n",
        "    processed_light = raw_data['light'] / 1000.0 # Simple normalization for light\n",
        "\n",
        "    print(\"Preprocessing complete for all modalities.\")\n",
        "    return {\n",
        "        'X_video': processed_video,\n",
        "        'X_accel': processed_accel,\n",
        "        'X_gyro': processed_gyro,\n",
        "        'X_light': processed_light\n",
        "    }\n",
        "\n",
        "# --- Consolidated Execution ---\n",
        "raw_sensors = acquire_multimodal_sensor_data()\n",
        "processed_sensors = preprocess_multimodal_data(raw_sensors)\n",
        "\n",
        "# Display summary of final prepared data\n",
        "for key, val in processed_sensors.items():\n",
        "    print(f\"Dataset: {key} | Shape: {val.shape} | Range: [{val.min():.2f}, {val.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e39c95c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Verify the entire notebook flow by running a comprehensive test of the data simulation, preprocessing, model inference, and evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "299546b1"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "\n",
        "**How was the 'AttributeError: bool object has no attribute astype' resolved?**\n",
        "The error was resolved by replacing boolean indexing with `np.where(predictions_test > 0.5, 1, 0)`. This ensures the output is a NumPy integer array rather than a boolean object, allowing for proper numerical operations and metric calculations.\n",
        "\n",
        "**How was the data dimension mismatch between predictions and test labels fixed?**\n",
        "The evaluation logic was refactored to explicitly call `model.predict(X_test)` immediately before calculating metrics. This ensures that the number of predicted samples (90) perfectly matches the ground truth labels in `y_test`, preventing `ValueError` during evaluation.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Model Performance:** The model currently exhibits baseline performance on the simulated dataset with an **Accuracy of 0.5000** and a **Test Loss of 0.7074**.\n",
        "*   **Classification Metrics:** Detailed evaluation on the 90-sample test set showed a **Recall of 1.0000** and an **F1-Score of 0.6667**, indicating the model is currently biased toward the positive class in the simulation.\n",
        "*   **Multimodal Data Simulation:** The consolidated pipeline successfully generates synchronized data:\n",
        "    *   **RGB Camera:** 150 frames (224x224x3).\n",
        "    *   **IMU Sensors:** 500 samples of Accelerometer and Gyroscope data (5 seconds at 100Hz).\n",
        "    *   **Ambient Light:** 500 samples normalized to a $[0, 1]$ range.\n",
        "*   **Pipeline Robustness:** The implementation of a verification block ensures that `model`, `X_test`, and `y_test` are present in the environment before execution, providing specific guidance if variables are missing.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Address Class Bias:** The high recall ($1.0000$) paired with baseline accuracy ($0.5000$) suggests the model may be predicting the majority class. Future steps should include balancing the simulated dataset or adjusting the decision threshold.\n",
        "*   **Sensor Fusion:** With the multimodal pipeline now established, the next logical step is to integrate the IMU and Light sensor data into the model architecture to move beyond image-only classification.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}